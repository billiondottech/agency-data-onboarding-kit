{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "header"
      },
      "source": [
        "# üßπ Learn Polars Data Cleaning - Interactive Notebook\n",
        "\n",
        "Welcome to the **Agency Data Onboarding Kit** learning experience!\n",
        "\n",
        "In this notebook, you'll learn how to clean messy client data using **Polars** - a blazing-fast data manipulation library. By the end, you'll understand exactly how to:\n",
        "\n",
        "- üìä Load and inspect messy CSV files\n",
        "- üßº Normalize column names and values\n",
        "- üîç Extract domains from website URLs\n",
        "- üåç Standardize country names\n",
        "- üìû Clean phone numbers\n",
        "- üóëÔ∏è Remove duplicates intelligently\n",
        "- ‚úÖ Filter out invalid data\n",
        "- üíæ Export clean data\n",
        "\n",
        "---\n",
        "\n",
        "## üéØ How to Use This Notebook\n",
        "\n",
        "1. **Run each cell in order** (Shift+Enter or click the Play button)\n",
        "2. **Read the explanations** before each code block\n",
        "3. **Experiment!** Change values and see what happens\n",
        "4. **Use your own data** in the final section\n",
        "\n",
        "**Time commitment:** 15-20 minutes\n",
        "\n",
        "---\n",
        "\n",
        "## üìö What is Polars?\n",
        "\n",
        "Polars is a modern data manipulation library that:\n",
        "- Runs 5-10x faster than Pandas\n",
        "- Uses less memory\n",
        "- Has syntax that reads like plain English\n",
        "- Handles messy data gracefully\n",
        "\n",
        "Think of it as \"Excel formulas that actually make sense.\"\n",
        "\n",
        "---\n",
        "\n",
        "**Ready? Let's dive in!** üöÄ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "install"
      },
      "source": [
        "## Step 1: Install Polars\n",
        "\n",
        "First, we need to install the Polars library. This only takes a few seconds."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "install-code"
      },
      "outputs": [],
      "source": [
        "!pip install polars -q\n",
        "\n",
        "print(\"‚úÖ Polars installed successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "import"
      },
      "source": [
        "## Step 2: Import Libraries\n",
        "\n",
        "Let's import Polars and set up our environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "import-code"
      },
      "outputs": [],
      "source": [
        "import polars as pl\n",
        "import re\n",
        "from datetime import datetime\n",
        "\n",
        "# Configure Polars to show more rows in output\n",
        "pl.Config.set_tbl_rows(20)\n",
        "\n",
        "print(\"‚úÖ Libraries imported!\")\n",
        "print(f\"üì¶ Polars version: {pl.__version__}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "load"
      },
      "source": [
        "## Step 3: Load Sample Data\n",
        "\n",
        "We'll download a sample messy CSV file from the repository. This is what a typical client sends: chaotic, inconsistent, and full of duplicates.\n",
        "\n",
        "**What's in this file:**\n",
        "- 41 contact records\n",
        "- Duplicate emails with different data completeness\n",
        "- Mixed phone formats\n",
        "- Country variations (UK, GB, United Kingdom, etc.)\n",
        "- Invalid emails\n",
        "- Missing data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "load-code"
      },
      "outputs": [],
      "source": [
        "# Download sample messy contacts CSV\n",
        "url = \"https://raw.githubusercontent.com/billion-community/agency-data-onboarding-kit/main/samples/contacts_messy.csv\"\n",
        "\n",
        "# Load the CSV\n",
        "df_raw = pl.read_csv(url)\n",
        "\n",
        "print(f\"‚úÖ Loaded {len(df_raw)} rows\")\n",
        "print(f\"\\nüìã Columns: {df_raw.columns}\")\n",
        "print(\"\\nüîç First 5 rows:\")\n",
        "df_raw.head(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "inspect"
      },
      "source": [
        "### üëÄ Inspect the Chaos\n",
        "\n",
        "Look at the data above. Notice:\n",
        "- **Trailing spaces** in column names (`\"Title \"`, `\"Country \"`)\n",
        "- **Mixed case emails** (SARAH.JOHNSON@... vs sarah.johnson@...)\n",
        "- **Different phone formats** ((555) 123-4567 vs +1-555-234-5678)\n",
        "- **Country variations** (USA, United States, US)\n",
        "\n",
        "Let's fix all of this automatically! üí™"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "normalize-cols"
      },
      "source": [
        "## Step 4: Normalize Column Names\n",
        "\n",
        "First issue: column names have trailing spaces and inconsistent formatting.\n",
        "\n",
        "**Goal:** Convert all columns to lowercase with underscores (snake_case)\n",
        "\n",
        "**Example:** `\"Full Name\"` ‚Üí `\"full_name\"`, `\"Title \"` ‚Üí `\"title\"`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "normalize-cols-code"
      },
      "outputs": [],
      "source": [
        "# Before: see the messy column names\n",
        "print(\"‚ùå BEFORE:\")\n",
        "print(df_raw.columns)\n",
        "\n",
        "# Normalize column names\n",
        "df = df_raw.rename({\n",
        "    col: col.strip().lower().replace(\" \", \"_\") \n",
        "    for col in df_raw.columns\n",
        "})\n",
        "\n",
        "# After: clean column names\n",
        "print(\"\\n‚úÖ AFTER:\")\n",
        "print(df.columns)\n",
        "\n",
        "print(\"\\nüéâ All columns are now clean and consistent!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "explain-cols"
      },
      "source": [
        "### üí° What Just Happened?\n",
        "\n",
        "```python\n",
        "col.strip()           # Remove leading/trailing spaces\n",
        "   .lower()           # Convert to lowercase\n",
        "   .replace(\" \", \"_\") # Replace spaces with underscores\n",
        "```\n",
        "\n",
        "This is a **dictionary comprehension** - it creates a mapping of old names ‚Üí new names for all columns at once."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "clean-email"
      },
      "source": [
        "## Step 5: Clean Email Addresses\n",
        "\n",
        "Emails should always be:\n",
        "- Lowercase (sarah.johnson@acme.com, not SARAH.JOHNSON@ACME.COM)\n",
        "- Trimmed (no leading/trailing spaces)\n",
        "\n",
        "Let's fix this with Polars!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "clean-email-code"
      },
      "outputs": [],
      "source": [
        "# See the problem\n",
        "print(\"‚ùå BEFORE - Mixed case emails:\")\n",
        "print(df.select([\"full_name\", \"email\"]).head(3))\n",
        "\n",
        "# Clean emails\n",
        "df = df.with_columns([\n",
        "    pl.col(\"email\").str.to_lowercase().str.strip().alias(\"email\")\n",
        "])\n",
        "\n",
        "print(\"\\n‚úÖ AFTER - All lowercase:\")\n",
        "print(df.select([\"full_name\", \"email\"]).head(3))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "explain-email"
      },
      "source": [
        "### üí° Understanding Polars Syntax\n",
        "\n",
        "```python\n",
        "df.with_columns([          # Add or modify columns\n",
        "    pl.col(\"email\")         # Select the email column\n",
        "      .str.to_lowercase()   # Make it lowercase\n",
        "      .str.strip()          # Remove spaces\n",
        "      .alias(\"email\")       # Keep the same column name\n",
        "])\n",
        "```\n",
        "\n",
        "This reads like English: \"Take the email column, make it lowercase, strip spaces, and save it back as email.\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "extract-domain"
      },
      "source": [
        "## Step 6: Extract Domains from Emails\n",
        "\n",
        "We need to know which company each contact belongs to. The domain in their email is a great indicator.\n",
        "\n",
        "**Example:** `sarah.johnson@acme-corp.com` ‚Üí `acme-corp.com`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "extract-domain-code"
      },
      "outputs": [],
      "source": [
        "# Extract domain from email\n",
        "df = df.with_columns([\n",
        "    pl.col(\"email\")\n",
        "      .str.split(\"@\")\n",
        "      .list.get(1)  # Get the part after @\n",
        "      .alias(\"email_domain\")\n",
        "])\n",
        "\n",
        "print(\"‚úÖ Domains extracted:\")\n",
        "print(df.select([\"email\", \"email_domain\"]).head(5))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "normalize-country"
      },
      "source": [
        "## Step 7: Normalize Country Names\n",
        "\n",
        "Look at the country column - it's a mess:\n",
        "- USA, United States, US\n",
        "- UK, United Kingdom, GB, U.K., uk\n",
        "\n",
        "Let's standardize these!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "normalize-country-code"
      },
      "outputs": [],
      "source": [
        "# First, see the problem\n",
        "print(\"‚ùå BEFORE - Country variations:\")\n",
        "print(df.group_by(\"country\").count().sort(\"count\", descending=True))\n",
        "\n",
        "# Define mapping\n",
        "country_map = {\n",
        "    \"usa\": \"United States\",\n",
        "    \"us\": \"United States\",\n",
        "    \"united states\": \"United States\",\n",
        "    \"uk\": \"United Kingdom\",\n",
        "    \"gb\": \"United Kingdom\",\n",
        "    \"united kingdom\": \"United Kingdom\",\n",
        "    \"u.k.\": \"United Kingdom\",\n",
        "}\n",
        "\n",
        "# Normalize countries\n",
        "df = df.with_columns([\n",
        "    pl.col(\"country\")\n",
        "      .str.to_lowercase()\n",
        "      .str.strip()\n",
        "      .replace(country_map)\n",
        "      .alias(\"country\")\n",
        "])\n",
        "\n",
        "print(\"\\n‚úÖ AFTER - Standardized:\")\n",
        "print(df.group_by(\"country\").count().sort(\"count\", descending=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "clean-phone"
      },
      "source": [
        "## Step 8: Clean Phone Numbers\n",
        "\n",
        "Phone numbers come in many formats. Let's keep only digits and the leading `+` sign."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "clean-phone-code"
      },
      "outputs": [],
      "source": [
        "# See the chaos\n",
        "print(\"‚ùå BEFORE - Phone number chaos:\")\n",
        "print(df.select([\"full_name\", \"phone\"]).head(10))\n",
        "\n",
        "# Clean phone numbers\n",
        "def clean_phone(phone):\n",
        "    if phone is None or phone == \"\":\n",
        "        return None\n",
        "    # Keep only digits and leading +\n",
        "    if phone.startswith(\"+\"):\n",
        "        return \"+\" + re.sub(r\"[^0-9]\", \"\", phone)\n",
        "    else:\n",
        "        return re.sub(r\"[^0-9]\", \"\", phone)\n",
        "\n",
        "df = df.with_columns([\n",
        "    pl.col(\"phone\").map_elements(clean_phone, return_dtype=pl.Utf8).alias(\"phone\")\n",
        "])\n",
        "\n",
        "print(\"\\n‚úÖ AFTER - Clean phone numbers:\")\n",
        "print(df.select([\"full_name\", \"phone\"]).head(10))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "clean-linkedin"
      },
      "source": [
        "## Step 9: Clean LinkedIn URLs\n",
        "\n",
        "Let's standardize LinkedIn URLs to a consistent format."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "clean-linkedin-code"
      },
      "outputs": [],
      "source": [
        "# Standardize LinkedIn URLs\n",
        "def clean_linkedin(url):\n",
        "    if url is None or url == \"\":\n",
        "        return None\n",
        "    url = url.lower()\n",
        "    url = url.replace(\"https://\", \"\").replace(\"http://\", \"\")\n",
        "    url = url.replace(\"www.\", \"\")\n",
        "    if not url.startswith(\"linkedin.com\"):\n",
        "        return None\n",
        "    return f\"https://{url}\"\n",
        "\n",
        "df = df.with_columns([\n",
        "    pl.col(\"linkedin\").map_elements(clean_linkedin, return_dtype=pl.Utf8).alias(\"linkedin\")\n",
        "])\n",
        "\n",
        "print(\"‚úÖ LinkedIn URLs standardized\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "filter-invalid"
      },
      "source": [
        "## Step 10: Filter Out Invalid Emails\n",
        "\n",
        "Some rows have invalid emails. Let's remove them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "filter-invalid-code"
      },
      "outputs": [],
      "source": [
        "print(f\"‚ùå BEFORE filtering: {len(df)} rows\")\n",
        "\n",
        "# Show invalid emails\n",
        "invalid = df.filter(\n",
        "    ~pl.col(\"email\").str.contains(\"@\") | \n",
        "    pl.col(\"email\").is_null()\n",
        ")\n",
        "print(f\"\\n‚ö†Ô∏è Found {len(invalid)} invalid emails\")\n",
        "\n",
        "# Filter them out\n",
        "df = df.filter(\n",
        "    pl.col(\"email\").str.contains(\"@\") & \n",
        "    pl.col(\"email\").is_not_null()\n",
        ")\n",
        "\n",
        "# Also filter out test/generic emails\n",
        "df = df.filter(\n",
        "    ~pl.col(\"email\").str.contains(\"test@\") &\n",
        "    ~pl.col(\"email\").str.starts_with(\"info@\")\n",
        ")\n",
        "\n",
        "print(f\"\\n‚úÖ AFTER filtering: {len(df)} rows\")\n",
        "print(f\"üóëÔ∏è Removed {len(df_raw) - len(df)} invalid rows\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "completeness"
      },
      "source": [
        "## Step 11: Calculate Data Completeness Score\n",
        "\n",
        "Let's score each row based on how much information it has."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "completeness-code"
      },
      "outputs": [],
      "source": [
        "# Calculate completeness score\n",
        "df = df.with_columns([\n",
        "    (\n",
        "        pl.col(\"full_name\").is_not_null().cast(pl.Int32) +\n",
        "        pl.col(\"email\").is_not_null().cast(pl.Int32) +\n",
        "        pl.col(\"title\").is_not_null().cast(pl.Int32) +\n",
        "        pl.col(\"phone\").is_not_null().cast(pl.Int32) +\n",
        "        pl.col(\"linkedin\").is_not_null().cast(pl.Int32)\n",
        "    ).alias(\"completeness_score\")\n",
        "])\n",
        "\n",
        "print(\"üìä Data Completeness Distribution:\")\n",
        "print(df.group_by(\"completeness_score\").count().sort(\"completeness_score\", descending=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "duplicates"
      },
      "source": [
        "## Step 12: Identify Duplicates\n",
        "\n",
        "Let's find duplicate emails in our dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "duplicates-code"
      },
      "outputs": [],
      "source": [
        "# Find duplicated emails\n",
        "duplicates = df.filter(pl.col(\"email\").is_duplicated())\n",
        "\n",
        "print(f\"‚ö†Ô∏è Found {len(duplicates)} duplicate email records\")\n",
        "if len(duplicates) > 0:\n",
        "    print(\"\\nüîç Duplicates (sorted by email):\")\n",
        "    print(duplicates.sort(\"email\").select([\n",
        "        \"email\", \"full_name\", \"title\", \"phone\", \"completeness_score\"\n",
        "    ]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "deduplicate"
      },
      "source": [
        "## Step 13: Deduplicate (Keep Best Record)\n",
        "\n",
        "Now the magic! We'll keep only the most complete record for each email."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "deduplicate-code"
      },
      "outputs": [],
      "source": [
        "print(f\"‚ùå BEFORE deduplication: {len(df)} rows\")\n",
        "\n",
        "# Deduplicate by email, keeping the most complete record\n",
        "df_clean = (\n",
        "    df.sort(\"completeness_score\", descending=True)\n",
        "      .unique(subset=[\"email\"], keep=\"first\")\n",
        "      .drop(\"completeness_score\")\n",
        ")\n",
        "\n",
        "print(f\"‚úÖ AFTER deduplication: {len(df_clean)} rows\")\n",
        "print(f\"üóëÔ∏è Removed {len(df) - len(df_clean)} duplicate records\")\n",
        "\n",
        "# Verify no more duplicates\n",
        "remaining_dupes = df_clean.filter(pl.col(\"email\").is_duplicated())\n",
        "print(f\"\\n‚ú® Remaining duplicates: {len(remaining_dupes)} (should be 0!)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "quality-check"
      },
      "source": [
        "## Step 14: Final Data Quality Check\n",
        "\n",
        "Let's see what we've accomplished!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "quality-check-code"
      },
      "outputs": [],
      "source": [
        "print(\"üìä FINAL DATA QUALITY REPORT\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "print(f\"\\n‚úÖ Total clean records: {len(df_clean)}\")\n",
        "print(f\"üìß All emails valid: {df_clean.filter(pl.col('email').str.contains('@')).height == len(df_clean)}\")\n",
        "print(f\"üîç No duplicate emails: {df_clean.filter(pl.col('email').is_duplicated()).height == 0}\")\n",
        "\n",
        "print(\"\\nüìà Field Completeness:\")\n",
        "for col in [\"full_name\", \"email\", \"title\", \"phone\", \"linkedin\", \"country\"]:\n",
        "    non_null = df_clean.filter(pl.col(col).is_not_null()).height\n",
        "    pct = (non_null / len(df_clean)) * 100\n",
        "    print(f\"  {col:15s}: {non_null:3d}/{len(df_clean):3d} ({pct:5.1f}%)\")\n",
        "\n",
        "print(\"\\nüåç Records by Country:\")\n",
        "print(df_clean.group_by(\"country\").count().sort(\"count\", descending=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "preview"
      },
      "source": [
        "## Step 15: Preview Clean Data\n",
        "\n",
        "Let's look at the final, beautiful, clean data!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "preview-code"
      },
      "outputs": [],
      "source": [
        "print(\"‚ú® YOUR CLEAN DATA (first 10 rows):\\n\")\n",
        "df_clean.head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "export"
      },
      "source": [
        "## Step 16: Export Clean Data\n",
        "\n",
        "Time to save your clean data!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "export-code"
      },
      "outputs": [],
      "source": [
        "# Export to CSV\n",
        "output_filename = f\"contacts_clean_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv\"\n",
        "df_clean.write_csv(output_filename)\n",
        "\n",
        "print(f\"‚úÖ Clean data exported to: {output_filename}\")\n",
        "print(f\"üì¶ File size: {len(df_clean)} rows x {len(df_clean.columns)} columns\")\n",
        "print(\"\\nüíæ Download the file from the files panel on the left ‚Üí\")\n",
        "\n",
        "print(\"\\nüìä TRANSFORMATION SUMMARY:\")\n",
        "print(\"=\" * 50)\n",
        "print(f\"  Original rows:        {len(df_raw)}\")\n",
        "print(f\"  Invalid filtered:     {len(df_raw) - len(df)}\")\n",
        "print(f\"  Duplicates removed:   {len(df) - len(df_clean)}\")\n",
        "print(f\"  Final clean rows:     {len(df_clean)}\")\n",
        "print(f\"  Data retained:        {(len(df_clean)/len(df_raw)*100):.1f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "learned"
      },
      "source": [
        "## üéì What You Just Learned\n",
        "\n",
        "Congratulations! You just cleaned messy client data like a pro.\n",
        "\n",
        "### Core Polars Skills\n",
        "‚úÖ Loading, transforming, filtering, sorting, deduplicating data  \n",
        "‚úÖ String operations and data type handling  \n",
        "‚úÖ Aggregations and quality checks  \n",
        "\n",
        "### Data Cleaning Techniques\n",
        "‚úÖ Normalization, validation, intelligent deduplication  \n",
        "\n",
        "---\n",
        "\n",
        "## üöÄ Next Steps\n",
        "\n",
        "1. ‚≠ê **Star the GitHub repo**: [agency-data-onboarding-kit](https://github.com/billion-community/agency-data-onboarding-kit)\n",
        "2. üí¨ **Join the WhatsApp community** - Share results, get help\n",
        "3. üî® **Build the full pipeline** - Add n8n + Supabase automation\n",
        "4. üìß **Subscribe to newsletter** - Advanced techniques\n",
        "\n",
        "---\n",
        "\n",
        "*Built with ‚ù§Ô∏è by the Billion community*"
      ]
    }
  ]
}