{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üßπ Learn Polars Data Cleaning - Interactive Notebook\n",
    "\n",
    "Welcome to the **Agency Data Onboarding Kit** learning experience!\n",
    "\n",
    "In this notebook, you'll learn how to clean messy client data using **Polars** - a blazing-fast data manipulation library. By the end, you'll understand exactly how to:\n",
    "\n",
    "- üìä Load and inspect messy CSV files\n",
    "- üßº Normalize column names and values\n",
    "- üîç Extract domains from website URLs\n",
    "- üåç Standardize country names\n",
    "- üìû Clean phone numbers\n",
    "- üóëÔ∏è Remove duplicates intelligently\n",
    "- ‚úÖ Filter out invalid data\n",
    "- üíæ Export clean data\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ How to Use This Notebook\n",
    "\n",
    "1. **Run each cell in order** (Shift+Enter or click the Play button)\n",
    "2. **Read the explanations** before each code block\n",
    "3. **Experiment!** Change values and see what happens\n",
    "4. **Use your own data** in the final section\n",
    "\n",
    "**Time commitment:** 15-20 minutes\n",
    "\n",
    "---\n",
    "\n",
    "## üìö What is Polars?\n",
    "\n",
    "Polars is a modern data manipulation library that:\n",
    "- Runs 5-10x faster than Pandas\n",
    "- Uses less memory\n",
    "- Has syntax that reads like plain English\n",
    "- Handles messy data gracefully\n",
    "\n",
    "Think of it as \"Excel formulas that actually make sense.\"\n",
    "\n",
    "---\n",
    "\n",
    "**Ready? Let's dive in!** üöÄ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Install Polars\n",
    "\n",
    "First, we need to install the Polars library. This only takes a few seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Polars\n",
    "!pip install polars -q\n",
    "\n",
    "print(\"‚úÖ Polars installed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Import Libraries\n",
    "\n",
    "Let's import Polars and set up our environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import re\n",
    "from datetime import datetime\n",
    "\n",
    "# Configure Polars to show more rows in output\n",
    "pl.Config.set_tbl_rows(20)\n",
    "\n",
    "print(\"‚úÖ Libraries imported!\")\n",
    "print(f\"üì¶ Polars version: {pl.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Load Sample Data\n",
    "\n",
    "We'll download a sample messy CSV file from the repository. This is what a typical client sends: chaotic, inconsistent, and full of duplicates.\n",
    "\n",
    "**What's in this file:**\n",
    "- 41 contact records\n",
    "- Duplicate emails with different data completeness\n",
    "- Mixed phone formats\n",
    "- Country variations (UK, GB, United Kingdom, etc.)\n",
    "- Invalid emails\n",
    "- Missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download sample messy contacts CSV\n",
    "url = \"https://raw.githubusercontent.com/billion-community/agency-data-onboarding-kit/main/samples/contacts_messy.csv\"\n",
    "\n",
    "# Load the CSV\n",
    "df_raw = pl.read_csv(url)\n",
    "\n",
    "print(f\"‚úÖ Loaded {len(df_raw)} rows\")\n",
    "print(f\"\\nüìã Columns: {df_raw.columns}\")\n",
    "print(\"\\nüîç First 5 rows:\")\n",
    "df_raw.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üëÄ Inspect the Chaos\n",
    "\n",
    "Look at the data above. Notice:\n",
    "- **Trailing spaces** in column names (`\"Title \"`, `\"Country \"`)\n",
    "- **Mixed case emails** (SARAH.JOHNSON@... vs sarah.johnson@...)\n",
    "- **Different phone formats** ((555) 123-4567 vs +1-555-234-5678)\n",
    "- **Country variations** (USA, United States, US)\n",
    "\n",
    "Let's fix all of this automatically! üí™"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Normalize Column Names\n",
    "\n",
    "First issue: column names have trailing spaces and inconsistent formatting.\n",
    "\n",
    "**Goal:** Convert all columns to lowercase with underscores (snake_case)\n",
    "\n",
    "**Example:** `\"Full Name\"` ‚Üí `\"full_name\"`, `\"Title \"` ‚Üí `\"title\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before: see the messy column names\n",
    "print(\"‚ùå BEFORE:\")\n",
    "print(df_raw.columns)\n",
    "\n",
    "# Normalize column names\n",
    "df = df_raw.rename({\n",
    "    col: col.strip().lower().replace(\" \", \"_\") \n",
    "    for col in df_raw.columns\n",
    "})\n",
    "\n",
    "# After: clean column names\n",
    "print(\"\\n‚úÖ AFTER:\")\n",
    "print(df.columns)\n",
    "\n",
    "print(\"\\nüéâ All columns are now clean and consistent!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üí° What Just Happened?\n",
    "\n",
    "```python\n",
    "col.strip()           # Remove leading/trailing spaces\n",
    "   .lower()           # Convert to lowercase\n",
    "   .replace(\" \", \"_\") # Replace spaces with underscores\n",
    "```\n",
    "\n",
    "This is a **dictionary comprehension** - it creates a mapping of old names ‚Üí new names for all columns at once."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Clean Email Addresses\n",
    "\n",
    "Emails should always be:\n",
    "- Lowercase (sarah.johnson@acme.com, not SARAH.JOHNSON@ACME.COM)\n",
    "- Trimmed (no leading/trailing spaces)\n",
    "\n",
    "Let's fix this with Polars!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See the problem\n",
    "print(\"‚ùå BEFORE - Mixed case emails:\")\n",
    "print(df.select([\"full_name\", \"email\"]).head(3))\n",
    "\n",
    "# Clean emails\n",
    "df = df.with_columns([\n",
    "    pl.col(\"email\").str.to_lowercase().str.strip().alias(\"email\")\n",
    "])\n",
    "\n",
    "print(\"\\n‚úÖ AFTER - All lowercase:\")\n",
    "print(df.select([\"full_name\", \"email\"]).head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üí° Understanding Polars Syntax\n",
    "\n",
    "```python\n",
    "df.with_columns([          # Add or modify columns\n",
    "    pl.col(\"email\")         # Select the email column\n",
    "      .str.to_lowercase()   # Make it lowercase\n",
    "      .str.strip()          # Remove spaces\n",
    "      .alias(\"email\")       # Keep the same column name\n",
    "])\n",
    "```\n",
    "\n",
    "This reads like English: \"Take the email column, make it lowercase, strip spaces, and save it back as email.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Extract Domains from Emails\n",
    "\n",
    "We need to know which company each contact belongs to. The domain in their email is a great indicator.\n",
    "\n",
    "**Example:** `sarah.johnson@acme-corp.com` ‚Üí `acme-corp.com`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract domain from email\n",
    "df = df.with_columns([\n",
    "    pl.col(\"email\")\n",
    "      .str.split(\"@\")\n",
    "      .list.get(1)  # Get the part after @\n",
    "      .alias(\"email_domain\")\n",
    "])\n",
    "\n",
    "print(\"‚úÖ Domains extracted:\")\n",
    "print(df.select([\"email\", \"email_domain\"]).head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üéØ Why This Matters\n",
    "\n",
    "Later, we'll use this domain to:\n",
    "- Link contacts to their company (account)\n",
    "- Detect duplicate companies\n",
    "- Enrich with company data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Normalize Country Names\n",
    "\n",
    "Look at the country column - it's a mess:\n",
    "- USA, United States, US\n",
    "- UK, United Kingdom, GB, U.K., uk\n",
    "\n",
    "Let's standardize these!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, see the problem\n",
    "print(\"‚ùå BEFORE - Country variations:\")\n",
    "print(df.group_by(\"country\").count().sort(\"count\", descending=True))\n",
    "\n",
    "# Define mapping\n",
    "country_map = {\n",
    "    \"usa\": \"United States\",\n",
    "    \"us\": \"United States\",\n",
    "    \"united states\": \"United States\",\n",
    "    \"uk\": \"United Kingdom\",\n",
    "    \"gb\": \"United Kingdom\",\n",
    "    \"united kingdom\": \"United Kingdom\",\n",
    "    \"u.k.\": \"United Kingdom\",\n",
    "}\n",
    "\n",
    "# Normalize countries\n",
    "df = df.with_columns([\n",
    "    pl.col(\"country\")\n",
    "      .str.to_lowercase()\n",
    "      .str.strip()\n",
    "      .replace(country_map)\n",
    "      .alias(\"country\")\n",
    "])\n",
    "\n",
    "print(\"\\n‚úÖ AFTER - Standardized:\")\n",
    "print(df.group_by(\"country\").count().sort(\"count\", descending=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üåç Result\n",
    "\n",
    "Now we only have **2 country values** instead of 8+ variations!\n",
    "\n",
    "This makes filtering and reporting much easier:\n",
    "```python\n",
    "df.filter(pl.col(\"country\") == \"United States\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Clean Phone Numbers\n",
    "\n",
    "Phone numbers come in many formats:\n",
    "- `(555) 123-4567`\n",
    "- `555-123-4567`\n",
    "- `+1-555-234-5678`\n",
    "- `+44 20 7123 4567`\n",
    "- `07700 900123`\n",
    "\n",
    "Let's keep only digits and the leading `+` sign."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See the chaos\n",
    "print(\"‚ùå BEFORE - Phone number chaos:\")\n",
    "print(df.select([\"full_name\", \"phone\"]).head(10))\n",
    "\n",
    "# Clean phone numbers\n",
    "def clean_phone(phone):\n",
    "    if phone is None or phone == \"\":\n",
    "        return None\n",
    "    # Keep only digits and leading +\n",
    "    if phone.startswith(\"+\"):\n",
    "        return \"+\" + re.sub(r\"[^0-9]\", \"\", phone)\n",
    "    else:\n",
    "        return re.sub(r\"[^0-9]\", \"\", phone)\n",
    "\n",
    "df = df.with_columns([\n",
    "    pl.col(\"phone\").map_elements(clean_phone, return_dtype=pl.Utf8).alias(\"phone\")\n",
    "])\n",
    "\n",
    "print(\"\\n‚úÖ AFTER - Clean phone numbers:\")\n",
    "print(df.select([\"full_name\", \"phone\"]).head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìû What Changed?\n",
    "\n",
    "- `(555) 123-4567` ‚Üí `5551234567`\n",
    "- `+1-555-234-5678` ‚Üí `+15552345678`\n",
    "- `+44 20 7123 4567` ‚Üí `+442071234567`\n",
    "\n",
    "Now all phones are in a consistent format!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Clean LinkedIn URLs\n",
    "\n",
    "LinkedIn URLs also need standardization:\n",
    "- Some have `https://www.`\n",
    "- Some have just `linkedin.com`\n",
    "- Some are missing the protocol\n",
    "\n",
    "Let's make them all consistent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See the variations\n",
    "print(\"‚ùå BEFORE - LinkedIn URL variations:\")\n",
    "print(df.select([\"full_name\", \"linkedin\"]).filter(pl.col(\"linkedin\").is_not_null()).head(5))\n",
    "\n",
    "# Standardize LinkedIn URLs\n",
    "def clean_linkedin(url):\n",
    "    if url is None or url == \"\":\n",
    "        return None\n",
    "    # Remove protocol and www\n",
    "    url = url.lower()\n",
    "    url = url.replace(\"https://\", \"\").replace(\"http://\", \"\")\n",
    "    url = url.replace(\"www.\", \"\")\n",
    "    # Add standard protocol\n",
    "    if not url.startswith(\"linkedin.com\"):\n",
    "        return None\n",
    "    return f\"https://{url}\"\n",
    "\n",
    "df = df.with_columns([\n",
    "    pl.col(\"linkedin\").map_elements(clean_linkedin, return_dtype=pl.Utf8).alias(\"linkedin\")\n",
    "])\n",
    "\n",
    "print(\"\\n‚úÖ AFTER - Standardized LinkedIn URLs:\")\n",
    "print(df.select([\"full_name\", \"linkedin\"]).filter(pl.col(\"linkedin\").is_not_null()).head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10: Filter Out Invalid Emails\n",
    "\n",
    "Some rows have invalid emails:\n",
    "- Missing the `@` symbol\n",
    "- Generic emails like `info@`, `test@`\n",
    "- Empty email fields\n",
    "\n",
    "Let's remove these!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See how many rows we have\n",
    "print(f\"‚ùå BEFORE filtering: {len(df)} rows\")\n",
    "\n",
    "# Show invalid emails\n",
    "invalid = df.filter(\n",
    "    ~pl.col(\"email\").str.contains(\"@\") | \n",
    "    pl.col(\"email\").is_null()\n",
    ")\n",
    "print(f\"\\n‚ö†Ô∏è Found {len(invalid)} invalid emails:\")\n",
    "print(invalid.select([\"full_name\", \"email\"]))\n",
    "\n",
    "# Filter them out\n",
    "df = df.filter(\n",
    "    pl.col(\"email\").str.contains(\"@\") & \n",
    "    pl.col(\"email\").is_not_null()\n",
    ")\n",
    "\n",
    "# Also filter out test/generic emails\n",
    "df = df.filter(\n",
    "    ~pl.col(\"email\").str.contains(\"test@\") &\n",
    "    ~pl.col(\"email\").str.starts_with(\"info@\")\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úÖ AFTER filtering: {len(df)} rows\")\n",
    "print(f\"üóëÔ∏è Removed {len(df_raw) - len(df)} invalid rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 11: Calculate Data Completeness Score\n",
    "\n",
    "Not all rows have the same amount of information. Some contacts have:\n",
    "- Full name + email + title + phone + LinkedIn ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ\n",
    "- Just name + email ‚úÖ‚úÖ\n",
    "\n",
    "Let's calculate a \"completeness score\" for each row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate completeness score (1 point for each filled field)\n",
    "df = df.with_columns([\n",
    "    (\n",
    "        pl.col(\"full_name\").is_not_null().cast(pl.Int32) +\n",
    "        pl.col(\"email\").is_not_null().cast(pl.Int32) +\n",
    "        pl.col(\"title\").is_not_null().cast(pl.Int32) +\n",
    "        pl.col(\"phone\").is_not_null().cast(pl.Int32) +\n",
    "        pl.col(\"linkedin\").is_not_null().cast(pl.Int32)\n",
    "    ).alias(\"completeness_score\")\n",
    "])\n",
    "\n",
    "# Show distribution\n",
    "print(\"üìä Data Completeness Distribution:\")\n",
    "print(df.group_by(\"completeness_score\").count().sort(\"completeness_score\", descending=True))\n",
    "\n",
    "print(\"\\nüîç Most complete records:\")\n",
    "print(df.sort(\"completeness_score\", descending=True).select([\n",
    "    \"full_name\", \"email\", \"title\", \"phone\", \"linkedin\", \"completeness_score\"\n",
    "]).head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üéØ Why This Matters\n",
    "\n",
    "When we find duplicates, we'll keep the record with the **highest completeness score**.\n",
    "\n",
    "Example: If we have 2 rows for sarah.johnson@acme-corp.com:\n",
    "- Row 1: Name + Email (score: 2)\n",
    "- Row 2: Name + Email + Title + Phone (score: 4)\n",
    "\n",
    "We'll keep Row 2!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 12: Identify Duplicates\n",
    "\n",
    "Let's find duplicate emails in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find duplicated emails\n",
    "duplicates = df.filter(pl.col(\"email\").is_duplicated())\n",
    "\n",
    "print(f\"‚ö†Ô∏è Found {len(duplicates)} duplicate email records:\")\n",
    "print(\"\\nüîç Duplicates (sorted by email):\")\n",
    "print(duplicates.sort(\"email\").select([\n",
    "    \"email\", \"full_name\", \"title\", \"phone\", \"completeness_score\"\n",
    "]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üëÄ Notice the Pattern\n",
    "\n",
    "See how some emails appear multiple times? Often:\n",
    "- One record has minimal info\n",
    "- Another has more complete data\n",
    "\n",
    "Let's fix this by keeping only the best record for each email!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 13: Deduplicate (Keep Best Record)\n",
    "\n",
    "Now the magic happens! We'll:\n",
    "1. Sort by completeness score (highest first)\n",
    "2. Keep only the first occurrence of each email\n",
    "\n",
    "This means we automatically keep the most complete record."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count before deduplication\n",
    "print(f\"‚ùå BEFORE deduplication: {len(df)} rows\")\n",
    "\n",
    "# Deduplicate by email, keeping the most complete record\n",
    "df_clean = (\n",
    "    df.sort(\"completeness_score\", descending=True)\n",
    "      .unique(subset=[\"email\"], keep=\"first\")\n",
    "      .drop(\"completeness_score\")  # Don't need this column anymore\n",
    ")\n",
    "\n",
    "# Count after\n",
    "print(f\"‚úÖ AFTER deduplication: {len(df_clean)} rows\")\n",
    "print(f\"üóëÔ∏è Removed {len(df) - len(df_clean)} duplicate records\")\n",
    "\n",
    "# Verify no more duplicates\n",
    "remaining_dupes = df_clean.filter(pl.col(\"email\").is_duplicated())\n",
    "print(f\"\\n‚ú® Remaining duplicates: {len(remaining_dupes)} (should be 0!)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üéâ Success!\n",
    "\n",
    "We've removed all duplicates while keeping the richest data for each contact!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 14: Final Data Quality Check\n",
    "\n",
    "Let's see what we've accomplished!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üìä FINAL DATA QUALITY REPORT\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Overall stats\n",
    "print(f\"\\n‚úÖ Total clean records: {len(df_clean)}\")\n",
    "print(f\"üìß All emails are valid: {df_clean.filter(pl.col('email').str.contains('@')).height == len(df_clean)}\")\n",
    "print(f\"üîç No duplicate emails: {df_clean.filter(pl.col('email').is_duplicated()).height == 0}\")\n",
    "\n",
    "# Field completeness\n",
    "print(\"\\nüìà Field Completeness:\")\n",
    "for col in [\"full_name\", \"email\", \"title\", \"phone\", \"linkedin\", \"country\"]:\n",
    "    non_null_count = df_clean.filter(pl.col(col).is_not_null()).height\n",
    "    percentage = (non_null_count / len(df_clean)) * 100\n",
    "    print(f\"  {col:15s}: {non_null_count:3d}/{len(df_clean):3d} ({percentage:5.1f}%)\")\n",
    "\n",
    "# Country breakdown\n",
    "print(\"\\nüåç Records by Country:\")\n",
    "print(df_clean.group_by(\"country\").count().sort(\"count\", descending=True))\n",
    "\n",
    "# Domain breakdown\n",
    "print(\"\\nüè¢ Top 5 Companies (by domain):\")\n",
    "print(df_clean.group_by(\"email_domain\").count().sort(\"count\", descending=True).head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 15: Preview Clean Data\n",
    "\n",
    "Let's look at the final, beautiful, clean data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"‚ú® YOUR CLEAN DATA (first 10 rows):\")\n",
    "print(\"\\n\")\n",
    "df_clean.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 16: Export Clean Data\n",
    "\n",
    "Time to save your clean data! You can download it as a CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export to CSV\n",
    "output_filename = f\"contacts_clean_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv\"\n",
    "df_clean.write_csv(output_filename)\n",
    "\n",
    "print(f\"‚úÖ Clean data exported to: {output_filename}\")\n",
    "print(f\"üì¶ File size: {len(df_clean)} rows x {len(df_clean.columns)} columns\")\n",
    "print(\"\\nüíæ Download the file from the files panel on the left ‚Üí\")\n",
    "\n",
    "# Show summary stats\n",
    "print(\"\\nüìä TRANSFORMATION SUMMARY:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"  Original rows:        {len(df_raw)}\")\n",
    "print(f\"  Invalid filtered:     {len(df_raw) - len(df)}\")\n",
    "print(f\"  Duplicates removed:   {len(df) - len(df_clean)}\")\n",
    "print(f\"  Final clean rows:     {len(df_clean)}\")\n",
    "print(f\"  Data retained:        {(len(df_clean)/len(df_raw)*100):.1f}%\")"